{
 "metadata": {
  "name": "",
  "signature": "sha256:7cc36f047fa9a24b3bf2c1960c5f4944dc741cd78c0b4adf81daf96f20814d01"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The purpose  of this file is to convert the crawled data (which contains unicode) into a simplified version, where each page is prepresented by a numeric string"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Load the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import json\n",
      "import GraphFunctions as gh\n",
      "import os\n",
      "import imp\n",
      "data_path = os.path.join('..','data','data.p')\n",
      "data_folder = os.path.join('..','data')\n",
      "with open(data_path, 'rb') as fp:\n",
      "    data = pickle.load(fp)\n",
      "len(data) == 77432"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's consider only the links for now\n",
      "only_links = {page : data[page]['links'] for page in data}\n",
      "len(only_links) == 77432"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Find missing links"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def missing_links(start, graph = only_links, depth = 5, max_links = 20, save = False, folder = \"data\"):\n",
      "    \"\"\"\n",
      "    Counts (and optionally saves in the folder \\'data\\') the missing page missed in the crawl.\n",
      "    Such pages are obtained by traversing the graph from \\'start\\' \n",
      "    up to depth \\'depth\\' following \\'max_links\\' links\n",
      "    \"\"\"\n",
      "    reduced_graph = gh.reduce_graph(graph, start, depth, max_links=max_links)\n",
      "    missing = [link for link in reduced_graph if link not in graph]\n",
      "    print (\"there are \", len(missing) , \" missing links for \", start )\n",
      "    if save:\n",
      "        with open(os.path.join(folder,'missing_links.p', 'wb')) as fp:\n",
      "            pickle.dump(missing, fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "missing_links('Donald_Duck', max_links = 15)\n",
      "missing_links('X-Men', max_links = 15)\n",
      "missing_links('Electronic_Arts', max_links = 15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "there are  46449  missing links for  Donald_Duck\n",
        "there are "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 23236  missing links for  X-Men\n",
        "there are "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 31381  missing links for  Electronic_Arts\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Clean the db"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_graph(data, folder = data_folder):\n",
      "    \"\"\"\n",
      "    Given a dictionary \\'data\\', returns a new one where strings are replaced by numbers.\n",
      "    The pairings \n",
      "        string : number \n",
      "    and \n",
      "        number : string \n",
      "    are saved on disk\n",
      "    \"\"\"\n",
      "    import os\n",
      "    \n",
      "    page_to_number_path = os.path.join(folder, \"page_to_number.json\")\n",
      "    number_to_page_path = os.path.join(folder, \"number_to_page.json\")\n",
      "    #Load previous results\n",
      "    try :\n",
      "        with open(page_to_number_path,'rt') as fp:\n",
      "            page_to_number = json.load(fp)\n",
      "        with open(number_to_page_path,'rt') as fp:\n",
      "            \n",
      "            number_to_page = json.load(fp)\n",
      "            count = max((int(key) for key in number_to_page.keys())) + 1\n",
      "            \n",
      "    except FileNotFoundError:\n",
      "            page_to_number = {}\n",
      "            number_to_page = {}\n",
      "            count = 1\n",
      "    \n",
      "    cleaned = {}\n",
      "    \n",
      "    # node here is a string\n",
      "    for node in data:\n",
      "\n",
      "        if node in page_to_number:\n",
      "            node_num = page_to_number[node]     \n",
      "        else:\n",
      "            node_num = count\n",
      "            page_to_number[node] = node_num\n",
      "            number_to_page[node_num] = node\n",
      "            count += 1\n",
      "            \n",
      "        cleaned[node_num] = []\n",
      "        \n",
      "        # note that link is a Node object, so we need to pull out the actual links from it\n",
      "        for linkObject in data[node].sons:\n",
      "            link = linkObject.key\n",
      "            \n",
      "            if link in page_to_number:\n",
      "                link_num = page_to_number[link]\n",
      "                \n",
      "            else:\n",
      "                link_num = count\n",
      "                page_to_number[link] = link_num\n",
      "                number_to_page[link_num] = link\n",
      "                count += 1                \n",
      "            cleaned[node_num].append(link_num)\n",
      "            \n",
      "    # save updated results\n",
      "    with open(page_to_number_path,'wt') as fp:\n",
      "        json.dump(page_to_number,fp)\n",
      "    with open(number_to_page_path,'wt') as fp:\n",
      "        json.dump(number_to_page,fp)\n",
      "        \n",
      "    #Check that everything worked fine\n",
      "    assert len(number_to_page) == len(page_to_number), \"Error in conversion\"\n",
      "    \n",
      "    \n",
      "    return cleaned"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Utils"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pageQ(page):\n",
      "    \"\"\"\n",
      "    given a Wikipeida page, returns the number corresponding to it in the JSON file\n",
      "    \"\"\"\n",
      "    with open(os.path.join(data_folder, \"page_to_number.json\"),'rt') as fp:\n",
      "        page_to_number = json.load(fp)\n",
      "    try:\n",
      "        return page_to_number[page]\n",
      "    except KeyError:\n",
      "        return None\n",
      "    \n",
      "def numberQ(number):\n",
      "    \"\"\"\n",
      "    given a number, it return the corresponding Wikipedia page\n",
      "    \"\"\"\n",
      "    with open(os.path.join(data_folder, \"number_to_page.json\"),'rt') as fp:\n",
      "        number_to_page = json.load(fp)\n",
      "    try:\n",
      "        return number_to_page[number]\n",
      "    except KeyError:\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pageQ('Python_(programming_language)') == 68714 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 96,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numberQ('10543') == 'X-Men'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 97,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Putting everything together"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reduce_and_clean(start, data = only_links, depth = 5, max_links = 15, save = False, folder = data_folder, clean = True):\n",
      "    \"\"\"\n",
      "    Given the full dictionray \\'data\\', it return a reduced version (starting from \\'start\\', with a max depth of \\'depth\\' and following the first \\'max_links\\')\n",
      "    that will be cleaned (and saved if necessary)\n",
      "    \"\"\"\n",
      "    #importing missing modules\n",
      "    import os\n",
      "    import json\n",
      "    import GraphFunctions as gh\n",
      "    \n",
      "    #reduce the graph\n",
      "    reduced = gh.reduce_graph(data, start, depth, max_links)\n",
      "    \n",
      "    #optionally clean the graph\n",
      "    if clean:\n",
      "        cleaned = clean_graph(reduced)\n",
      "    else:\n",
      "        cleaned = reduced\n",
      "           \n",
      "    print (\"There are \", len(cleaned), \" elements\")\n",
      "\n",
      "    #optinally save the graph to file\n",
      "    if save:\n",
      "        name = \"_\".join([start, str(depth), str(max_links)])+\".json\"\n",
      "        print (\"Saving file \",name)\n",
      "        with open(os.path.join(folder, name), 'wt') as fp:\n",
      "            json.dump(cleaned, fp)\n",
      "            \n",
      "    return cleaned\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduce_and_clean(\"Donald_Duck\", max_links=20, depth=5, save = True, clean = True);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are  149526  elements\n",
        "Saving file  Donald_Duck_5_20.json\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Clean the entire graph"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Note: this is currently broken, need a iter method for Node class\n",
      "entire_cleaned = clean_graph(only_links)\n",
      "len(entire_cleaned) == 77432"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'list' object has no attribute 'sons'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-103-1818aee4e432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentire_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentire_cleaned\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m77432\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-94-fa47bbbbfc50>\u001b[0m in \u001b[0;36mclean_graph\u001b[0;34m(data, folder)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# note that link is a Node object, so we need to pull out the actual links from it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlinkObject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sons'"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(os.path.join(data_folder,'cleaned.json'), 'wt') as fp:\n",
      "    json.dump(entire_cleaned, fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}