{
 "metadata": {
  "name": "",
  "signature": "sha256:f92e24cdfe3817a1fcaa12de38f23d112c93efe396808884ed7006d707cf14c4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The purpose  of this file is to convert the crawled data (which contains unicode) into a simplified version, where each page is prepresented by a numeric string"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Load the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "import json\n",
      "import GraphHelpers as gh\n",
      "import os\n",
      "import imp\n",
      "with open(os.path.join('data','data.p'), 'rb') as fp:\n",
      "    data = pickle.load(fp)\n",
      "len(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "77432"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's consider only the links for now\n",
      "only_links = {page : data[page]['links'] for page in data}\n",
      "len(only_links)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "77432"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Find missing links"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def missing_links(start, graph = only_links, depth = 5, max_links = 20, save = False, folder = \"data\"):\n",
      "    \"\"\"\n",
      "    Counts (and optionally saves in the folder \\'data\\') the missing page missed in the crawl.\n",
      "    Such pages are obtained by traversing the graph from \\'start\\' \n",
      "    up to depth \\'depth\\' following \\'max_links\\' links\n",
      "    \"\"\"\n",
      "    reduced_graph = gh.reduce_graph(graph, start, depth, max_links=max_links)\n",
      "    missing = [link for link in reduced_graph if link not in graph]\n",
      "    print (\"there are \", len(missing) , \" missing links for \", start )\n",
      "    if save:\n",
      "        with open(os.path.join(folder,'missing_links.p', 'wb')) as fp:\n",
      "            pickle.dump(missing, fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "missing_links('Donald_Duck', max_links = 15)\n",
      "missing_links('X-Men', max_links = 15)\n",
      "missing_links('Electronic_Arts', max_links = 15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "there are  46449  missing links for  Donald_Duck\n",
        "there are "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 23236  missing links for  X-Men\n",
        "there are "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 31381  missing links for  Electronic_Arts\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Clean the db"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_graph(data, folder = \"data\"):\n",
      "    \"\"\"\n",
      "    Given a dictionary \\'data\\', returns a new one where strings are replaced by numbers.\n",
      "    The pairings \n",
      "        string : number \n",
      "    and \n",
      "        number : string \n",
      "    are saved on disk\n",
      "    \"\"\"\n",
      "    \n",
      "    import os\n",
      "    \n",
      "    page_to_number_path = os.path.join(folder, \"page_to_number.p\")\n",
      "    number_to_page_path = os.path.join(folder, \"number_to_page.p\")\n",
      "    #Load previous results\n",
      "    try :\n",
      "        with open(page_to_number_path,'rb') as fp:\n",
      "            page_to_number = pickle.load(fp)\n",
      "        with open(number_to_page_path,'rb') as fp:\n",
      "            number_to_page = pickle.load(fp)\n",
      "            count = max(number_to_page.keys()) + 1\n",
      "            \n",
      "    except FileNotFoundError:\n",
      "            page_to_number = {}\n",
      "            number_to_page = {}\n",
      "            count = 1\n",
      "    \n",
      "    cleaned = {}\n",
      "    \n",
      "    for node in data:\n",
      "        \n",
      "        if node in page_to_number:\n",
      "            node_num = page_to_number[node]     \n",
      "        else:\n",
      "            node_num = count\n",
      "            page_to_number[node] = node_num\n",
      "            number_to_page[node_num] = node\n",
      "            count += 1\n",
      "            \n",
      "        cleaned[node_num] = []\n",
      "        \n",
      "        for link in data[node]:\n",
      "            \n",
      "            if link in page_to_number:\n",
      "                link_num = page_to_number[link]\n",
      "                \n",
      "            else:\n",
      "                link_num = count\n",
      "                page_to_number[link] = link_num\n",
      "                number_to_page[link_num] = link\n",
      "                count += 1                \n",
      "            cleaned[node_num].append(link_num)\n",
      "            \n",
      "    #Save updated results\n",
      "    with open(page_to_number_path,'wb') as fp:\n",
      "        pickle.dump(page_to_number,fp)\n",
      "    with open(number_to_page_path,'wb') as fp:\n",
      "        pickle.dump(number_to_page,fp)\n",
      "        \n",
      "    #Check that everything worked fine\n",
      "    assert len(number_to_page) == len(page_to_number), \"Error in conversion\"\n",
      "    \n",
      "    \n",
      "    return cleaned"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Utils"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pageQ(page):\n",
      "    \"\"\"\n",
      "    given a Wikipeida page, returns the number corresponding to it in the JSON file\n",
      "    \"\"\"\n",
      "    with open(os.path.join('data', \"page_to_number.p\"),'rb') as fp:\n",
      "        page_to_number = pickle.load(fp)\n",
      "    try:\n",
      "        return page_to_number[page]\n",
      "    except KeyError:\n",
      "        return None\n",
      "    \n",
      "def numberQ(number):\n",
      "    \"\"\"\n",
      "    given a number, it return the corresponding Wikipedia page\n",
      "    \"\"\"\n",
      "    with open(os.path.join('data', \"number_to_page.p\"),'rb') as fp:\n",
      "        number_to_page = pickle.load(fp)\n",
      "    try:\n",
      "        return number_to_page[number]\n",
      "    except KeyError:\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pageQ('Donald_Duck')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "383"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numberQ(4241)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "'List_of_metropolitan_areas_of_the_United_States'"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Putting everything together"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reduce_and_clean(start, data = only_links, depth = 5, max_links = 15, save = False, folder = 'data', clean = True):\n",
      "    \"\"\"\n",
      "    Given the full dictionray \\'data\\', it return a reduced version (starting from \\'start\\', with a max depth of \\'depth\\' and following the first \\'max_links\\')\n",
      "    that will be cleaned (and saved if necessary)\n",
      "    \"\"\"\n",
      "    #importing missing modules\n",
      "    import os\n",
      "    import json\n",
      "    import GraphHelpers as gh\n",
      "    \n",
      "    #reduce the graph\n",
      "    reduced = gh.reduce_graph(data, start, depth, max_links)\n",
      "    \n",
      "    #optionally clean the graph\n",
      "    if clean:\n",
      "        cleaned = clean_graph(reduced)\n",
      "    else:\n",
      "        cleaned = reduced\n",
      "           \n",
      "    print (\"There are \", len(cleaned), \" elements\")\n",
      "\n",
      "    #optinally save the graph to file\n",
      "    if save:\n",
      "        name = \"_\".join([start, str(depth), str(max_links)])+\".json\"\n",
      "        print (\"Saving file \",name)\n",
      "        with open(os.path.join(folder, name), 'wt') as fp:\n",
      "            json.dump(cleaned, fp)\n",
      "            \n",
      "    return cleaned\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduce_and_clean(\"Donald_Duck\", max_links=10, depth=5, save = True, clean = True);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are  3548  elements\n",
        "Saving file  Donald_Duck_5_10.json\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Clean the entire graph"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entire_cleaned = clean_graph(only_links)\n",
      "len(entire_cleaned)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "77432"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(os.path.join('data','cleaned.json'), 'wt') as fp:\n",
      "    json.dump(entire_cleaned, fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}